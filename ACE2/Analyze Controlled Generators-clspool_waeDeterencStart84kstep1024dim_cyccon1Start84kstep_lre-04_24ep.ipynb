{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, MT5Config, MT5EncoderModel, MT5Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from transformers_custom import MT5ForConditionalGenerationWithLatentSpace\n",
    "from progeny_tokenizer import TAPETokenizer\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import scipy\n",
    "import time\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, RandomSampler, Dataset, BatchSampler\n",
    "import typing\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 30\n",
    "data_dir = \"data/gen_train_data/top_half_ddG\"\n",
    "pretrained_dir = \"./congen/v1/clspool_waeDeterencStart84kstep1024dim_cyccon1Start84kstep_lre-04_24ep/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_json = '/export/share/bkrause/progen/progeny/t5_base_uniref_bfd50/config.json'\n",
    "# shutil.copy(src_json, pretrained_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "output_dir = Path(\"./congen/v1/analysis/clspool_waeDeterencStart84kstep1024dim_cyccon1Start84kstep_lre-04_24ep/\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tokenizer = TAPETokenizer(vocab=\"progeny\")\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "t5config = MT5Config.from_pretrained(pretrained_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_args = {\n",
    "    'latent_pooler': 'cls',\n",
    "    'pool_enc_hidden_states_for_dec': True,\n",
    "    'latent_space_type': 'wae',\n",
    "    'mask_non_target_z_vector': False,\n",
    "    'separate_targetattr_head': False,\n",
    "    'z_tar_vector_dim': 1,\n",
    "    'do_mi': False,\n",
    "    'latent_size': 1024,\n",
    "    'wae_z_enc_type': 'deterministic',\n",
    "    'separate_latent_enc': False,\n",
    "    'separate_latent_dec': False,\n",
    "}\n",
    "\n",
    "model = MT5ForConditionalGenerationWithLatentSpace.from_pretrained(pretrained_dir, **latent_space_args)\n",
    "\n",
    "model.parallelize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-diameter",
   "metadata": {},
   "source": [
    "# Evaluate Generator's ddG predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio=0.9\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PKLDFDatasetForGen(Dataset):\n",
    "    \"\"\"Creates a dataset from an pkl df file.\n",
    "    Args:\n",
    "        data_file (typing.Union[str, Path]): Path to pkl df file.\n",
    "        in_memory (bool, optional): Whether to load the full dataset into memory.\n",
    "            Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                data_file: typing.Union[str, Path],\n",
    "                in_memory: bool = False,\n",
    "                split: str = 'train',\n",
    "                train_ratio: float = 1,\n",
    "                train_data_file: str = '250K_ddG_split/train_ddG.pkl',\n",
    "                data_subset='full'\n",
    "                ):\n",
    "\n",
    "        data_file = Path(data_file)\n",
    "        if not data_file.exists():\n",
    "            raise FileNotFoundError(data_file)\n",
    "        \n",
    "        df = pd.read_pickle(data_file)\n",
    "        \n",
    "        if train_ratio != 1:\n",
    "            shuffled_df = df.sort_index()\n",
    "            # shuffled_df = df.sample(frac=1)\n",
    "            train_num_samples = int(len(shuffled_df) * train_ratio)\n",
    "            if split == 'train':\n",
    "                final_df = shuffled_df.iloc[:train_num_samples]\n",
    "            elif split == 'valid':\n",
    "                final_df = shuffled_df.iloc[train_num_samples:]\n",
    "            else:\n",
    "                final_df = df\n",
    "        else:\n",
    "            final_df = df\n",
    "        \n",
    "        # split into subset if not full training set\n",
    "        if data_subset != 'full':\n",
    "            ddG_sorted_final_df = final_df.sort_values(by='ddG', ascending=True)\n",
    "            train_subset_num_samples = int( data_subset * len(ddG_sorted_final_df) ) \n",
    "            final_df = ddG_sorted_final_df.iloc[:train_subset_num_samples]\n",
    "\n",
    "        print(\"split: \", split)\n",
    "        print(\"data_file: \", data_file)\n",
    "        print(\"len(final_df): \", len(final_df))\n",
    "\n",
    "        self.df = final_df\n",
    "        num_examples = len(final_df)\n",
    "        self._num_examples = num_examples\n",
    "        \n",
    "        if in_memory:\n",
    "            cache = [None] * num_examples\n",
    "            self._cache = cache\n",
    "            \n",
    "        self._in_memory = in_memory\n",
    "\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self._num_examples\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if not 0 <= index < self._num_examples:\n",
    "            raise IndexError(index)\n",
    "\n",
    "        if self._in_memory and self._cache[index] is not None:\n",
    "            item = self._cache[index]\n",
    "        else:\n",
    "            row = self.df.iloc[index]\n",
    "            item = {}\n",
    "            item['ddG'] = row['ddG'] #!\n",
    "            item['input_ids'] = row['MT_seq'] #!\n",
    "            item['labels'] = row['MT_seq']\n",
    "\n",
    "            item['id'] = str(index)\n",
    "            if self._in_memory:\n",
    "                self._cache[index] = item\n",
    "            \n",
    "        return item\n",
    "\n",
    "def pad_sequences(sequences: typing.Sequence, constant_value=0, dtype=None) -> np.ndarray:\n",
    "    batch_size = len(sequences)\n",
    "    shape = [batch_size] + np.max([seq.shape for seq in sequences], 0).tolist()\n",
    "\n",
    "    if dtype is None:\n",
    "        dtype = sequences[0].dtype\n",
    "\n",
    "    if isinstance(sequences[0], np.ndarray):\n",
    "        array = np.full(shape, constant_value, dtype=dtype)\n",
    "    elif isinstance(sequences[0], torch.Tensor):\n",
    "        array = torch.full(shape, constant_value, dtype=dtype)\n",
    "\n",
    "    for arr, seq in zip(array, sequences):\n",
    "        arrslice = tuple(slice(dim) for dim in seq.shape)\n",
    "        arr[arrslice] = seq\n",
    "\n",
    "    return array\n",
    "\n",
    "class CustomStabilityDatasetForGenLatentSpace(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                data_path: typing.Union[str, Path],\n",
    "                split: str,\n",
    "                tokenizer: typing.Union[str, TAPETokenizer] = 'iupac',\n",
    "                in_memory: bool = False,\n",
    "                train_ratio: float = 1,\n",
    "                normalize_targets: bool = False,\n",
    "                data_subset='full'):\n",
    "\n",
    "        # if split not in ('train', 'valid', 'test'):\n",
    "        #     raise ValueError(f\"Unrecognized split: {split}. \"\n",
    "        #                     f\"Must be one of ['train', 'valid', 'test']\")\n",
    "        if isinstance(tokenizer, str):\n",
    "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if split == 'valid':\n",
    "            file_prefix = 'train'\n",
    "        else:\n",
    "            file_prefix = split\n",
    "            \n",
    "        data_path = Path(data_path)\n",
    "        data_file = f'{file_prefix}_ddG.pkl' \n",
    "\n",
    "        self.data = PKLDFDatasetForGen(data_path / data_file, in_memory, split, train_ratio, data_subset='full')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.data[index]\n",
    "        input_ids = self.tokenizer.encode(item['input_ids']) \n",
    "        labels = self.tokenizer.encode(item['labels'])\n",
    "        ddG = item['ddG']\n",
    "        return input_ids, labels, ddG\n",
    "\n",
    "    \n",
    "    def collate_fn(self, batch: typing.List[typing.Tuple[typing.Any, ...]]) -> typing.Dict[str, torch.Tensor]:\n",
    "        input_ids, labels, ddG = tuple(zip(*batch))\n",
    "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
    "        labels = torch.from_numpy(pad_sequences(labels, 0))\n",
    "        ddG = torch.Tensor(ddG)\n",
    "\n",
    "        return {'input_ids': input_ids,\n",
    "                'labels': labels,\n",
    "                'ddG': ddG}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomStabilityDatasetForGenLatentSpace(data_dir, 'train', train_ratio=train_ratio, tokenizer=tokenizer)\n",
    "eval_dataset = CustomStabilityDatasetForGenLatentSpace(data_dir, 'valid', train_ratio=train_ratio, tokenizer=tokenizer)\n",
    "\n",
    "# Train data set-up\n",
    "train_loader = DataLoader(train_dataset, batch_size=per_device_train_batch_size, shuffle=True, \n",
    "                        num_workers=0, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "train_loader = tqdm(train_loader)\n",
    "\n",
    "# Eval data set-up\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=per_device_eval_batch_size, shuffle=False, \n",
    "                        num_workers=0, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "\n",
    "eval_loader = tqdm(eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearmanr(target, prediction):\n",
    "    target_array = np.asarray(target)\n",
    "    prediction_array = np.asarray(prediction)\n",
    "    print(\"target_array.shape: \", target_array.shape)\n",
    "    print(\"prediction_array.shape: \", prediction_array.shape)\n",
    "    return scipy.stats.spearmanr(target_array, prediction_array).correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def evaluate(model, eval_iterator, do_mi=False, do_ddG_spearmanr=True, latent_space_type='plain', return_pred=False):\n",
    "    eval_contrastive_loss_total = 0\n",
    "    eval_lm_loss_total = 0\n",
    "    if do_mi:\n",
    "        eval_mi_head_loss_total = 0\n",
    "    if latent_space_type == 'vae':\n",
    "        eval_kl_loss_total = 0\n",
    "    model.eval()\n",
    "    num_eval_batch = 0\n",
    "    \n",
    "    ddG_preds=[]\n",
    "    ddG_targs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(eval_iterator):\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            labels = batch['labels'].to(model.device)\n",
    "            ddG_targets = batch['ddG'].to(model.device)\n",
    "            \n",
    "            if do_mi:\n",
    "                model_outputs = model(input_ids, labels=labels, contrast_targets=ddG_targets)\n",
    "                outputs, contrastive_loss, contrastive_value, mi_head_loss = model_outputs[0], model_outputs[1], model_outputs[2], model_outputs[3]\n",
    "                eval_mi_head_loss_total = eval_mi_head_loss_total + mi_head_loss\n",
    "            else:\n",
    "                model_outputs = model(input_ids, labels=labels, contrast_targets=ddG_targets)\n",
    "                outputs, contrastive_loss, contrastive_value = model_outputs[0], model_outputs[1], model_outputs[2]\n",
    "            \n",
    "            if latent_space_type == 'vae':\n",
    "                kl_loss = model_outputs[-1]\n",
    "\n",
    "            for pred, target in zip(contrastive_value.squeeze().cpu().numpy(), ddG_targets.cpu().numpy()):\n",
    "#                 print(\"target: \", target)\n",
    "#                 print(\"pred: \", pred)\n",
    "                ddG_targs.append(target)\n",
    "                ddG_preds.append(pred)\n",
    "\n",
    "            lm_loss = outputs.loss\n",
    "            \n",
    "            eval_contrastive_loss_total = eval_contrastive_loss_total + contrastive_loss\n",
    "            eval_lm_loss_total = eval_lm_loss_total + lm_loss\n",
    "\n",
    "            if latent_space_type == 'vae':\n",
    "                eval_kl_loss_total = eval_kl_loss_total + kl_loss\n",
    "            \n",
    "            # eval_contrastive_losses.append(contrastive_loss)\n",
    "            # eval_lm_losses.append(lm_loss)\n",
    "\n",
    "            num_eval_batch += 1\n",
    "\n",
    "#             if step == 5:\n",
    "#                 break\n",
    "\n",
    "    # eval_contrastive_loss = torch.mean(eval_contrastive_losses)\n",
    "    # eval_lm_loss = torch.mean(eval_lm_losses)\n",
    "    eval_lm_loss = eval_lm_loss_total / num_eval_batch\n",
    "    eval_contrastive_loss = eval_contrastive_loss_total / num_eval_batch\n",
    "    eval_output = {\n",
    "                \"lm_loss\": eval_lm_loss,\n",
    "                \"contrastive_loss\": eval_contrastive_loss,\n",
    "                  }\n",
    "\n",
    "    if do_mi:\n",
    "        eval_mi_head_loss_total = eval_mi_head_loss_total / num_eval_batch\n",
    "        eval_output['mi_head_loss'] = eval_mi_head_loss_total\n",
    "\n",
    "    if latent_space_type == 'vae':\n",
    "        eval_kl_loss_total = eval_kl_loss_total / num_eval_batch\n",
    "        eval_output['kl_loss'] = eval_kl_loss_total\n",
    "\n",
    "    if do_ddG_spearmanr:\n",
    "        spearmanr_value = spearmanr(ddG_targs, ddG_preds)\n",
    "        print(\"spearmanr_value: \", spearmanr_value)\n",
    "        eval_output['spearmanr'] = spearmanr_value\n",
    "    \n",
    "    if return_pred:\n",
    "        eval_output['ddG_preds'] = ddG_preds\n",
    "        eval_output['ddG_targs'] = ddG_targs\n",
    "\n",
    "\n",
    "    # print(\"eval_contrastive_loss: \", eval_contrastive_loss)\n",
    "    # print(\"eval_lm_loss: \", eval_lm_loss)\n",
    "    return eval_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, eval_iterator, do_mi=False, do_ddG_spearmanr=True, return_pred=False):\n",
    "#     eval_contrastive_loss_total = 0\n",
    "#     eval_lm_loss_total = 0\n",
    "#     if do_mi:\n",
    "#         eval_mi_head_loss_total = 0\n",
    "#     model.eval()\n",
    "#     num_eval_batch = 0\n",
    "    \n",
    "#     ddG_preds=[]\n",
    "#     ddG_targs = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for step, batch in enumerate(eval_iterator):\n",
    "            \n",
    "#             input_ids = batch['input_ids'].to(model.device)\n",
    "#             labels = batch['labels'].to(model.device)\n",
    "#             ddG_targets = batch['ddG'].to(model.device)\n",
    "            \n",
    "#             if do_mi:\n",
    "#                 outputs, contrastive_loss, contrastive_value, mi_head_loss = model(input_ids, labels=labels, contrast_targets=ddG_targets)\n",
    "#                 eval_mi_head_loss_total = eval_mi_head_loss_total + mi_head_loss\n",
    "#             else:\n",
    "#                 outputs, contrastive_loss, contrastive_value = model(input_ids, labels=labels, contrast_targets=ddG_targets)\n",
    "            \n",
    "#             for pred, target in zip(contrastive_value.squeeze().cpu().numpy(), ddG_targets.cpu().numpy()):\n",
    "# #                 print(\"target: \", target)\n",
    "# #                 print(\"pred: \", pred)\n",
    "#                 ddG_targs.append(target)\n",
    "#                 ddG_preds.append(pred)\n",
    "                \n",
    "# #             ddG_targs.append(torch.flatten(ddG_targets).cpu().numpy())\n",
    "# #             ddG_preds.append(torch.flatten(contrastive_value).squeeze().cpu().numpy())\n",
    "\n",
    "# #             print(\"ddG_targets.shape: \", ddG_targets.cpu().numpy().shape)\n",
    "# #             print(\"contrastive_value.shape: \", contrastive_value.cpu().numpy().shape)\n",
    "#             lm_loss = outputs.loss\n",
    "            \n",
    "#             eval_contrastive_loss_total = eval_contrastive_loss_total + contrastive_loss\n",
    "#             eval_lm_loss_total = eval_lm_loss_total + lm_loss\n",
    "#             # eval_contrastive_losses.append(contrastive_loss)\n",
    "#             # eval_lm_losses.append(lm_loss)\n",
    "\n",
    "#             num_eval_batch += 1\n",
    "\n",
    "# #             if step == 5:\n",
    "# #                 break\n",
    "\n",
    "#     # eval_contrastive_loss = torch.mean(eval_contrastive_losses)\n",
    "#     # eval_lm_loss = torch.mean(eval_lm_losses)\n",
    "#     eval_lm_loss = eval_lm_loss_total / num_eval_batch\n",
    "#     eval_contrastive_loss = eval_contrastive_loss_total / num_eval_batch\n",
    "#     eval_output = {\n",
    "#                 \"lm_loss\": eval_lm_loss,\n",
    "#                 \"contrastive_loss\": eval_contrastive_loss,\n",
    "#                   }\n",
    "\n",
    "#     if do_mi:\n",
    "#         eval_mi_head_loss_total = eval_mi_head_loss_total / num_eval_batch\n",
    "#         eval_output['mi_head_loss'] = eval_mi_head_loss_total\n",
    "\n",
    "#     if do_ddG_spearmanr:\n",
    "#         spearmanr_value = spearmanr(ddG_targs, ddG_preds)\n",
    "#         print(\"spearmanr_value: \", spearmanr_value)\n",
    "#         eval_output['spearmanr'] = spearmanr_value\n",
    "        \n",
    "#     if return_pred:\n",
    "#         eval_output['ddG_preds'] = ddG_preds\n",
    "#         eval_output['ddG_targs'] = ddG_targs\n",
    "        \n",
    "    \n",
    "#     # print(\"eval_contrastive_loss: \", eval_contrastive_loss)\n",
    "#     # print(\"eval_lm_loss: \", eval_lm_loss)\n",
    "#     return eval_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-thought",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_output = evaluate(model, eval_loader, do_mi=latent_space_args['do_mi'], return_pred=True, latent_space_type=latent_space_args['latent_space_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_lm_loss, eval_contrastive_loss, eval_spearmanr_value = eval_output['lm_loss'], eval_output['contrastive_loss'], eval_output['spearmanr']\n",
    "\n",
    "print(\"eval_lm_loss: \", eval_lm_loss)\n",
    "print(\"eval_contrastive_loss: \", eval_contrastive_loss)\n",
    "print(\"eval_spearmanr_value: \", eval_spearmanr_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddG_preds, ddG_targs = eval_output['ddG_preds'], eval_output['ddG_targs']\n",
    "\n",
    "print(\"len(ddG_preds): \", len(ddG_preds))\n",
    "print(\"len(ddG_targs): \", len(ddG_targs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"stats of ddG_preds\")\n",
    "print(\"min: \", np.min(ddG_preds))\n",
    "print(\"mean: \", np.mean(ddG_preds))\n",
    "print(\"median: \", np.median(ddG_preds))\n",
    "print(\"max: \", np.max(ddG_preds))\n",
    "print(\"std: \", np.std(ddG_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(ddG_preds, density=True, label='value_pred', bins=[i for i in range(-20, 10)], alpha=0.4)\n",
    "\n",
    "\n",
    "plt.hist(ddG_targs, density=True, label='ddG', bins=[i for i in range(-20, 10)], alpha=0.4)\n",
    "plt.xlabel(\"ddG\", size=14)\n",
    "plt.ylabel(\"Density\", size=14)\n",
    "plt.title(\"Eval set, Controlled generation clspool_waeDeterencStart84kstep1024dim_cyccon1Start84kstep_lre-04_24ep\")\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['value_pred'] = ddG_preds\n",
    "eval_df['ddG'] = ddG_targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = eval_df.sort_values(by='value_pred', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topK_list = [10, 100, 1000, 10000]\n",
    "percentile_list = [95, 90, 85, 80, 75]\n",
    "topK_list = [len(eval_df)*(100-i)//100 for i in percentile_list]\n",
    "print(topK_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddG_df = eval_df\n",
    "all_ddG_list = eval_df['ddG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-conditioning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topK in topK_list:\n",
    "    topK_df = ddG_df[:topK]\n",
    "    print(\"top K: \", len(topK_df))\n",
    "    print(\"max: \", np.max(topK_df['ddG']))\n",
    "    print(\"min: \", np.min(topK_df['ddG']))\n",
    "    print(\"mean: \", np.mean(topK_df['ddG']))\n",
    "    print(\"median: \", np.median(topK_df['ddG']))\n",
    "    \n",
    "    for percentile in percentile_list:\n",
    "        pct = np.percentile(all_ddG_list, 100-percentile)\n",
    "        PCI_pct = np.sum(topK_df['ddG'] < pct) / len(topK_df['ddG'])\n",
    "        print(\"PCI_{}pct: \".format(percentile), PCI_pct)\n",
    "    \n",
    "    PCI_WT = np.sum(topK_df['ddG'] < 0) / len(topK_df['ddG'])\n",
    "    print(\"PCI_WT: \", PCI_WT)\n",
    "    \n",
    "    print(\"_\"*20)\n",
    "    \n",
    "tophalf_df = ddG_df[:len(ddG_df)//2]\n",
    "print(\"top half: \", len(tophalf_df))\n",
    "print(\"max: \", np.max(tophalf_df['ddG']))\n",
    "print(\"min: \", np.min(tophalf_df['ddG']))\n",
    "print(\"mean: \", np.mean(tophalf_df['ddG']))\n",
    "print(\"median: \", np.median(tophalf_df['ddG']))\n",
    "\n",
    "\n",
    "# PCI_75pct = np.sum(tophalf_df['ddG'] < train_75pct) / len(tophalf_df['ddG'])\n",
    "# print(\"PCI_75pct: \", PCI_75pct)\n",
    "\n",
    "for percentile in percentile_list:\n",
    "    pct = np.percentile(all_ddG_list, 100-percentile)\n",
    "    PCI_pct = np.sum(tophalf_df['ddG'] < pct) / len(tophalf_df['ddG'])\n",
    "    print(\"PCI_{}pct: \".format(percentile), PCI_pct)\n",
    "\n",
    "\n",
    "PCI_WT = np.sum(tophalf_df['ddG'] < 0) / len(tophalf_df['ddG'])\n",
    "print(\"PCI_WT: \", PCI_WT)\n",
    "\n",
    "print(\"_\"*20)\n",
    "\n",
    "\n",
    "# training data distribution\n",
    "print(\"train dataset: \", len(all_ddG_list))\n",
    "print(\"max: \", np.max(all_ddG_list))\n",
    "print(\"min: \", np.min(all_ddG_list))\n",
    "print(\"mean: \", np.mean(all_ddG_list))\n",
    "print(\"median: \", np.median(all_ddG_list))\n",
    "\n",
    "\n",
    "for percentile in percentile_list:\n",
    "    pct = np.percentile(all_ddG_list, 100-percentile)\n",
    "    PCI_pct = np.sum(all_ddG_list < pct) / len(all_ddG_list)\n",
    "    print(\"PCI_{}pct: \".format(percentile), PCI_pct)\n",
    "\n",
    "\n",
    "PCI_WT = np.sum(all_ddG_list < 0) / len(all_ddG_list)\n",
    "print(\"PCI_WT: \", PCI_WT)\n",
    "\n",
    "print(\"_\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-trustee",
   "metadata": {},
   "source": [
    "# Get value_pred of gen input samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = Path(data_dir)\n",
    "input_data_file = f'train_ddG.pkl' \n",
    "input_data_file = input_data_path / input_data_file\n",
    "input_data_df = pd.read_pickle(input_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_as_input = 12500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ddG stats of input data\")\n",
    "print(\"min: \", np.min(input_data_df['ddG']))\n",
    "print(\"mean: \", np.mean(input_data_df['ddG']))\n",
    "print(\"median: \", np.median(input_data_df['ddG']))\n",
    "print(\"max: \", np.max(input_data_df['ddG']))\n",
    "\n",
    "ddG_sorted_input_df = input_data_df.sort_values(by='ddG', ascending=True)\n",
    "\n",
    "gen_input_df = ddG_sorted_input_df.iloc[:topk_as_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-reach",
   "metadata": {},
   "source": [
    "# Get value_pred of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-puzzle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_eval_output = evaluate(model, train_loader, do_mi=latent_space_args['do_mi'], return_pred=True,  latent_space_type=latent_space_args['latent_space_type'])\n",
    "# eval_output = evaluate(model, eval_loader, do_mi=latent_space_args['do_mi'], return_pred=True, latent_space_type=latent_space_args['latent_space_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lm_loss, train_contrastive_loss, train_spearmanr_value = train_eval_output['lm_loss'], train_eval_output['contrastive_loss'], train_eval_output['spearmanr']\n",
    "\n",
    "print(\"train_lm_loss: \", train_lm_loss)\n",
    "print(\"train_contrastive_loss: \", train_contrastive_loss)\n",
    "print(\"train_spearmanr_value: \", train_spearmanr_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ddG_preds, train_ddG_targs = train_eval_output['ddG_preds'], train_eval_output['ddG_targs']\n",
    "\n",
    "print(\"len(train_ddG_preds): \", len(train_ddG_preds))\n",
    "print(\"len(train_ddG_targs): \", len(train_ddG_targs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"stats of ddG_preds, train set\")\n",
    "print(\"min: \", np.min(train_ddG_preds))\n",
    "print(\"mean: \", np.mean(train_ddG_preds))\n",
    "print(\"median: \", np.median(train_ddG_preds))\n",
    "print(\"max: \", np.max(train_ddG_preds))\n",
    "print(\"std: \", np.std(train_ddG_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(train_ddG_preds, density=True, label='value_pred', bins=[i for i in range(-20, 10)], alpha=0.4)\n",
    "\n",
    "\n",
    "plt.hist(train_ddG_targs, density=True, label='ddG', bins=[i for i in range(-20, 10)], alpha=0.4)\n",
    "plt.xlabel(\"ddG\", size=14)\n",
    "plt.ylabel(\"Density\", size=14)\n",
    "plt.title(\"Train set, Controlled generation clspool_waeDeterencStart84kstep1024dim_cyccon1Start84kstep_lre-04_24ep\")\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-retreat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-projector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-fleece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-concrete",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
